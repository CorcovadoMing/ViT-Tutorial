{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18248037-5e50-476f-b7b7-53d231082be0",
   "metadata": {
    "id": "18248037-5e50-476f-b7b7-53d231082be0"
   },
   "source": [
    "# ViT implementation practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968538b-099c-45fd-9aa1-8bcee87ba766",
   "metadata": {
    "id": "1968538b-099c-45fd-9aa1-8bcee87ba766"
   },
   "source": [
    "#### 1. **Tokenizer**, which takes an image and splits into several non-overlapping patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0D0UL-9ZiBPG",
   "metadata": {
    "id": "0D0UL-9ZiBPG"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import einops\n",
    "except:\n",
    "    !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ad3c2",
   "metadata": {
    "id": "5b8ad3c2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class Image2Tokens(nn.Module):\n",
    "    def __init__(self, image_size, dim, in_dim=3, patch_size=16, emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = image_size\n",
    "        num_patches = (image_height // patch_size) * (image_width // patch_size)\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('#! >>> fill the correct einops statement here for prepare patches', p1=patch_size, p2=patch_size),\n",
    "            #! >>> fill the embedding layer declaration here (hint: transform from `patch_size * patch_size * in_dim` to `dim`\n",
    "        )\n",
    "        self.pos_embedding = #! >>> fill the modules declaration here\n",
    "        self.cls_token = #! >>> fill the modules declaration here\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "\n",
    "        #! >>> fill the neccessary codes here\n",
    "        # Steps:\n",
    "        #      1. Split the images into (patch_size x patch_size) non-overlapping patches\n",
    "        #      2. Apply a linear transformation to make the 2D patches into 1D vectors in given hidden size: `dim`\n",
    "        #      3. Append a class-token at the very beginning (index 0)\n",
    "        #      4. Add a learnable embeddings to every tokens (including the added class-token)\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a3543",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "924a3543",
    "outputId": "390ec8a3-10e7-49fc-8340-57c7515a709a"
   },
   "outputs": [],
   "source": [
    "tokenizer = Image2Tokens(image_size=(224,224), dim=768)\n",
    "tokenizer(torch.randn(1,3,224,224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856709e-fb91-4f7e-83df-f5993aa5f728",
   "metadata": {
    "id": "b856709e-fb91-4f7e-83df-f5993aa5f728"
   },
   "source": [
    "#### 2. **Multi-Head Self-Attention**. Implement the following equation: $Softmax(\\frac{QK^T}{\\sqrt{d}})V$, where Q, K, V are embedded representations of input, $Q=w_Qx, K=w_Kx, V=w_Vx$, in multi-head manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64b8d4",
   "metadata": {
    "id": "bf64b8d4"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = #! >>> fill the correct scale here\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, dim*3)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        \n",
    "        #! >>> fill the implementation to complete the self-attention implementations\n",
    "        # Hint: the q,k,v are already in multi-headed, with the shape: (batch, heads, num_tokens, dim).\n",
    "        \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1038acb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1038acb",
    "outputId": "5b424f48-6c41-4ec8-ada8-1a55c3c7f1e0"
   },
   "outputs": [],
   "source": [
    "attn_op = Attention(dim=768)\n",
    "attn_op(torch.randn(1,197,768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d925b9-cf07-4538-af6c-9b2ee30753fa",
   "metadata": {
    "id": "c3d925b9-cf07-4538-af6c-9b2ee30753fa"
   },
   "source": [
    "#### 3. **FeedForwardNetwork (FFN)**. Implement $FFN=w_2 GELU(w_1x + b_1) + b_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b47d3",
   "metadata": {
    "id": "568b47d3"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #! >>> fill the neccessary modules to complete the FFN module, note the w1 transforms the dim to 4*dim, and w2 transforms the intermidiate output from 4*dim back to dim.\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d80b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "284d80b9",
    "outputId": "7d83f474-82f2-4cea-a312-9b1a79e536ca"
   },
   "outputs": [],
   "source": [
    "ffn = FeedForwardNetwork(768)\n",
    "ffn(torch.randn(1,197,768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa38b4-20ec-4500-b0f1-b987d1fcfd07",
   "metadata": {
    "id": "fcfa38b4-20ec-4500-b0f1-b987d1fcfd07"
   },
   "source": [
    "#### 4. Implement the **transformer encoder**, using prenorm shortcut style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a853d56",
   "metadata": {
    "id": "3a853d56"
   },
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, layers, dim, heads=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForwardNetwork(dim, dropout=dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        #! >>> Implement the forward flow here, all the neccessary modules are already declared.\n",
    "        # Remember the return the result :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b8e6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "811b8e6e",
    "outputId": "4e6cba1d-7d93-49c8-9590-bf2ba17b51dc"
   },
   "outputs": [],
   "source": [
    "model = Transformer(layers=12, dim=768)\n",
    "model(torch.randn(1,197,768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a49b60-841d-4ca5-bb97-12443872655b",
   "metadata": {
    "id": "97a49b60-841d-4ca5-bb97-12443872655b"
   },
   "source": [
    "#### 5. Package the **ViT model** with hyper-parameters configurable: (i) number of layers; (ii) hidden size; (iii) number of multiheads; (iv) image size (for tokenizer); (v) classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba7494",
   "metadata": {
    "id": "dcba7494"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, layers, dim, heads, image_size, num_classes, patch_size=16, in_dim=3, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.tokenizer = Image2Tokens(#! >>> using the arguments to call the implemented components in previous cells\n",
    "        self.transformer = Transformer(#! >>> using the arguments to call the implemented components in previous cells\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        #! >>> Implement the forward flow here\n",
    "        # Note the classifier should accept only class-token, not all the tokens.              \n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e7237",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac7e7237",
    "outputId": "cb5d7bb4-2657-4c7a-f9c5-171857f89485"
   },
   "outputs": [],
   "source": [
    "model = ViT(layers=12, dim=768, heads=12, image_size=(224,224), num_classes=1000)\n",
    "model(torch.randn(1,3,224,224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f24f52e-eba8-4a8e-9031-a9e84debf763",
   "metadata": {
    "id": "9f24f52e-eba8-4a8e-9031-a9e84debf763"
   },
   "source": [
    "#### Sanity check the parameter counts are matches the numbers in the ViT paper: (i) 87M for ViT-Base; (ii) 304M for ViT-Large; (iii) 632M for ViT-Huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51347c7",
   "metadata": {
    "id": "b51347c7"
   },
   "outputs": [],
   "source": [
    "def num_of_parameters(model):\n",
    "    params = 0\n",
    "    for i in model.parameters():\n",
    "        params += i.numel()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57446d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab57446d",
    "outputId": "098a1e0d-6084-4047-b432-8ce5f9e19717"
   },
   "outputs": [],
   "source": [
    "vit_base = ViT(layers=12, dim=768, heads=12, image_size=(224,224), num_classes=1000)\n",
    "vit_large = ViT(layers=24, dim=1024, heads=16, image_size=(224,224), num_classes=1000)\n",
    "vit_huge = ViT(layers=32, dim=1280, heads=16, image_size=(224,224), num_classes=1000)\n",
    "\n",
    "print(num_of_parameters(vit_base))\n",
    "print(num_of_parameters(vit_large))\n",
    "print(num_of_parameters(vit_huge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caae36-ea32-4831-8867-acc30a87c3f5",
   "metadata": {},
   "source": [
    "#### The outputs should be:\n",
    "```\n",
    "86567656\n",
    "304326632\n",
    "632199400\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5d12c-4086-45f2-a4ad-070abf4f29ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4299e-7dae-4a29-8d47-f31db06d54ae",
   "metadata": {
    "id": "8ba4299e-7dae-4a29-8d47-f31db06d54ae"
   },
   "source": [
    "# Train the implemented ViT on \"Cat vs Dog Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ef6d2",
   "metadata": {
    "id": "a46ef6d2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def UnzipTrainSet():\n",
    "    with zipfile.ZipFile('train.zip') as train_zip:\n",
    "        train_zip.extractall('data_dogcat')\n",
    "        \n",
    "def GetList():\n",
    "    train_list = glob.glob(os.path.join('data_dogcat', 'train', '*.jpg'))\n",
    "    return train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdef03-99e4-4c10-b1d4-2de80dba8487",
   "metadata": {
    "id": "38cdef03-99e4-4c10-b1d4-2de80dba8487"
   },
   "source": [
    "#### Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b66aa8-3f16-4bf0-99c7-a9a728fbb66c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1b66aa8-3f16-4bf0-99c7-a9a728fbb66c",
    "outputId": "46721ad9-1344-4696-8584-18e5925864fb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('train.zip'):\n",
    "    !wget -O train.zip https://www.dropbox.com/s/wd28l8279yttbez/train.zip?dl=1\n",
    "\n",
    "try:\n",
    "    train_list = GetList()\n",
    "    assert len(train_list) == 25000\n",
    "except:\n",
    "    os.makedirs('data_dogcat', exist_ok=True)\n",
    "    UnzipTrainSet()\n",
    "    train_list = GetList()\n",
    "    assert len(train_list) == 25000\n",
    "\n",
    "labels = [path.split('/')[-1].split('.')[0] for path in train_list]\n",
    "split_train_list, split_val_list = train_test_split(train_list, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=labels)\n",
    "print(f\"Train Data: {len(split_train_list)}\")\n",
    "print(f\"Valid Data: {len(split_val_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1210dad-5796-4916-aaea-e08488b4499c",
   "metadata": {
    "id": "e1210dad-5796-4916-aaea-e08488b4499c"
   },
   "source": [
    "#### Dataloader and Augmentations (resize, horizontal flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56b959-b6d6-448e-b1cb-a2fe9ab95c2f",
   "metadata": {
    "id": "5d56b959-b6d6-448e-b1cb-a2fe9ab95c2f"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        label = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = 1 if label == \"dog\" else 0\n",
    "\n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa1547-5bdd-4a64-8b85-0239b0be4246",
   "metadata": {
    "id": "36aa1547-5bdd-4a64-8b85-0239b0be4246"
   },
   "source": [
    "## Training with implemented ViT \n",
    "\n",
    "#### To run the training we need least 10GB of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d354199-49a9-4c12-b180-8980a1b54373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6cce61aa267747fd873195f78fe2bc05",
      "cc558b4ad47b4290af37d95dcae1d145",
      "1e4cbd9ef4be41ba94f3426215fb5039",
      "fe0551c7d1314b4fad0033cd4bc32dfd",
      "e0d6068aa16945d6a590321751af6bcc",
      "e8216cb8be6f4326be9256a36459e112",
      "a79274e61d7346a1af82dfabd5b05abe",
      "bcf0009445214188b5916a1af6bb54c5",
      "afcaefee59504e1bba7b8f7373de9408",
      "a5b8adce725b43abafdb26ffe2d248ba",
      "d249bf3ebefb44c8826657cac1441440"
     ]
    },
    "id": "3d354199-49a9-4c12-b180-8980a1b54373",
    "outputId": "fd4c40d4-54af-4e96-9b5b-605ee6226873"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-6\n",
    "device = 'cuda'\n",
    "\n",
    "model = ViT(layers=9, dim=192, heads=12, image_size=(224, 224), num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "train_data = CatsDogsDataset(split_train_list, transform=train_transforms)\n",
    "val_data = CatsDogsDataset(split_val_list, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        \n",
    "    model.eval()    \n",
    "    with torch.inference_mode():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in tqdm(val_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(val_loader)\n",
    "            epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a921af-fae4-498b-8a8d-fb59aaca1284",
   "metadata": {
    "id": "c6a921af-fae4-498b-8a8d-fb59aaca1284"
   },
   "source": [
    "## (Optional) Using ImageNet pretrained weights, using `timm` library\n",
    "\n",
    "#### we use half batch size to fit the smallest pretrained model, ViT Base, however it still needs at least 12G GPU memory to run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f80a40-6ff7-44b6-b897-6d7dd8575b4e",
   "metadata": {
    "id": "c7f80a40-6ff7-44b6-b897-6d7dd8575b4e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import timm\n",
    "except:\n",
    "    !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea133bef-ac9d-4fdc-b6b1-69833745f753",
   "metadata": {
    "id": "ea133bef-ac9d-4fdc-b6b1-69833745f753"
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class PretrainedViT(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = timm.create_model('vit_base_patch16_224_in21k', True)\n",
    "        self.classifier = nn.Linear(768, num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model.forward_features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6b25f-ab78-4055-815d-3835b9b24555",
   "metadata": {
    "id": "6ed6b25f-ab78-4055-815d-3835b9b24555"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-6\n",
    "device = 'cuda'\n",
    "\n",
    "pretrained_model = PretrainedViT(2)\n",
    "pretrained_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    pretrained_model.train()\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = pretrained_model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "            \n",
    "    pretrained_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in tqdm(val_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = pretrained_model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(val_loader)\n",
    "            epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe29c2e-a023-4623-8370-909eedc17059",
   "metadata": {},
   "source": [
    "#### Since ViT is a data-hungry model, the significant accuracy improvement can be observed using large dataset pretrained weights. This suggests always consider using the pretrained model if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb108108-4051-450d-b49b-83e6638a8ac4",
   "metadata": {
    "id": "bb108108-4051-450d-b49b-83e6638a8ac4"
   },
   "source": [
    "# (Extra) Visual inspect the learned attention weights\n",
    "\n",
    "#### The following codes only applicable on our implemented ViT. Some modifications are needed for `timm` ViTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73263d-e222-40d1-9acd-022fbff3d80b",
   "metadata": {
    "id": "d47ca4cc-e4b3-48d1-a902-3be9a73c7414"
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def find_modules(nn_module, type):\n",
    "    return [module for module in nn_module.modules() if isinstance(module, type)]\n",
    "\n",
    "class Recorder(nn.Module):\n",
    "    def __init__(self, vit, device = None):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "\n",
    "        self.data = None\n",
    "        self.recordings = []\n",
    "        self.hooks = []\n",
    "        self.hook_registered = False\n",
    "        self.ejected = False\n",
    "        self.device = device\n",
    "\n",
    "    def _hook(self, _, input, output):\n",
    "        self.recordings.append(output.clone().detach())\n",
    "\n",
    "    def _register_hook(self):\n",
    "        modules = find_modules(self.vit.transformer, Attention)\n",
    "        for module in modules:\n",
    "            handle = module.attend.register_forward_hook(self._hook)\n",
    "            self.hooks.append(handle)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def eject(self):\n",
    "        self.ejected = True\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "        return self.vit\n",
    "\n",
    "    def clear(self):\n",
    "        self.recordings.clear()\n",
    "\n",
    "    def record(self, attn):\n",
    "        recording = attn.clone().detach()\n",
    "        self.recordings.append(recording)\n",
    "\n",
    "    def forward(self, img):\n",
    "        assert not self.ejected, 'recorder has been ejected, cannot be used anymore'\n",
    "        self.clear()\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        pred = self.vit(img)\n",
    "\n",
    "        # move all recordings to one device before stacking\n",
    "        target_device = self.device if self.device is not None else img.device\n",
    "        recordings = tuple(map(lambda t: t.to(target_device), self.recordings))\n",
    "\n",
    "        attns = torch.stack(recordings, dim = 1) if len(recordings) > 0 else None\n",
    "        return pred, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991deef-0727-4fb6-96a6-b9c6288f3003",
   "metadata": {
    "id": "5991deef-0727-4fb6-96a6-b9c6288f3003"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "patch_size = 16\n",
    "image_size = 224\n",
    "select_index = 0 # change the index to visualize different images\n",
    "\n",
    "model.cpu()\n",
    "model.eval()\n",
    "v = Recorder(model) # load our implemented ViT\n",
    "\n",
    "data, label = next(iter(val_loader))\n",
    "_, attns = v(data)\n",
    "\n",
    "m = attns[select_index][:, :, 0, 1:].sum(1)\n",
    "plt.imshow(data[select_index].permute(1,2,0))\n",
    "plt.imshow(cv2.resize(m.sum(0).reshape(image_size//patch_size, image_size//patch_size).numpy(), (image_size, image_size)), cmap='hot', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.title('Averaged Attention Weights')\n",
    "plt.show()\n",
    "\n",
    "_, arr = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for idx, i in enumerate(m):\n",
    "    arr[idx//3, idx%3].imshow(cv2.resize(i.reshape(image_size//patch_size, image_size//patch_size).numpy(), (image_size, image_size)), cmap='hot')\n",
    "    arr[idx//3, idx%3].axis('off')\n",
    "    arr[idx//3, idx%3].set_title(f'Attention Layer {idx}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab2e2e-e87d-4daf-af1d-b1af0c875884",
   "metadata": {},
   "source": [
    "#### Note we trained our implemented ViT for only 5 epochs. The visualization result may not be converged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf3307-9f42-416c-940f-b206b046c555",
   "metadata": {},
   "source": [
    "### Here we provide the pretrained model (trained for 100 epochs) for visualization. (accuracy 0.7613)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea60af2-8f48-4e1b-8e4f-e96c46bc13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model.pt'):\n",
    "    !wget https://www.dropbox.com/s/ffj5j1kdt30racl/model.pt?dl=1 # trained 100 epochs, acc: 0.7613\n",
    "\n",
    "\n",
    "patch_size = 16\n",
    "image_size = 224\n",
    "select_index = 0 # change the index to visualize different images\n",
    "\n",
    "model = ViT(layers=9, dim=192, heads=12, image_size=(224, 224), num_classes=2)\n",
    "model.load_state_dict(torch.load('model.pt')['weight'])\n",
    "model.cpu()\n",
    "model.eval()\n",
    "v = Recorder(model) # load our implemented ViT\n",
    "\n",
    "data, label = next(iter(val_loader))\n",
    "pred, attns = v(data)\n",
    "\n",
    "m = attns[select_index][:, :, 0, 1:].sum(1)\n",
    "plt.imshow(data[select_index].permute(1,2,0))\n",
    "plt.imshow(cv2.resize(m[-1].reshape(image_size//patch_size, image_size//patch_size).numpy(), (image_size, image_size)), cmap='hot', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.title('Averaged Attention Weights')\n",
    "plt.show()\n",
    "\n",
    "_, arr = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for idx, i in enumerate(m):\n",
    "    arr[idx//3, idx%3].imshow(cv2.resize(i.reshape(image_size//patch_size, image_size//patch_size).numpy(), (image_size, image_size)), cmap='hot')\n",
    "    arr[idx//3, idx%3].axis('off')\n",
    "    arr[idx//3, idx%3].set_title(f'Attention Layer {idx}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
